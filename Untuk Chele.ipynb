{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Language Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import ast\n",
    "from itertools import islice\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def join_tags(tokens):\n",
    "    for i in enumerate(tokens):\n",
    "        hit = 0\n",
    "        index = i[0]\n",
    "        y = i[1]\n",
    "        width = len(y)\n",
    "        \n",
    "        for x in y:\n",
    "            if x is '-':\n",
    "                #save = tokens[index][hit+1:]\n",
    "                #tokens[index] = tokens[index][0:hit] + ''\n",
    "                #tokens[index+1] = save\n",
    "                #print(save)\n",
    "                #print(tokens[index])\n",
    "                tokens[index] = tokens[index][0:hit] + ' ' + tokens[index][hit+1:]\n",
    "            hit += 1\n",
    "        \n",
    "        if tokens[index] is ' ':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index] == '  ':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if y[(width-1):width] is '.':\n",
    "            tokens[index] = y[0:(width-1)]\n",
    "        \n",
    "        if tokens[index][0:1] is '-' or tokens[index][1:2] is '-':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index][-2:-1] is \"'\":\n",
    "            tokens[index] = tokens[index][0:-2]\n",
    "        \n",
    "        if  tokens[index][:1] is \"'\" :\n",
    "            tokens[index] = tokens[index][1:]\n",
    "        \n",
    "        if tokens[index] is \"s\" or tokens[index] is \"re\":\n",
    "            tokens[index-1] = tokens[index-1]\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index] is '<' and tokens[index+2] is '>':\n",
    "            tokens[index+2] = '<'+ tokens[index+1]+'>'\n",
    "            tokens[index] =''\n",
    "            tokens[index+1] = ''\n",
    "            \n",
    "        if tokens[index] is '&' and tokens[index+2] is ';':\n",
    "            tokens[index+2] = ''\n",
    "            tokens[index] = ''\n",
    "            tokens[index+1] = ''\n",
    "            \n",
    "        if tokens[index] is '(' or tokens[index] is ')':\n",
    "            tokens[index] = ''\n",
    "            \n",
    "        if tokens[index][0:1] is ':':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index][0:1] is '.' and tokens[index][1:2] is '.':\n",
    "            tokens[index] = ''\n",
    "\n",
    "        if y[(width-2):(width-1)] == '.' and y[(width-1):(width)] == '0':\n",
    "            tokens[index] = y[0:(width-2)]\n",
    "        \n",
    "    tokens_clean = [x for x in tokens if x != '' and x not in \".,!?'\" and x not in '<>']\n",
    "        \n",
    "    return tokens_clean\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess(text_process):\n",
    "    text_process = text_process.lower()\n",
    "    tokens = word_tokenize(text_process)\n",
    "    tokens_clean = join_tags(tokens)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in tokens_clean if not w in stop_words]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stemmed_words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    #clean_text = lemmatized_text\n",
    "    clean_text = lemmatized_text\n",
    "    clean_words = sorted(list([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stemmed_words]))\n",
    "    clean_text = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><root>' + clean_text + '</root>'\n",
    "    return clean_text, clean_words\n",
    "\n",
    "\n",
    "def language_model(query, limit, lambd):\n",
    "    clean_query_text, clean_query_words = preprocess(query)\n",
    "#     lambd = 1/4\n",
    "#     limit = 10\n",
    "    result_dict = list()\n",
    "\n",
    "    df_doc_freq = pd.read_csv('Document Frequencies.csv')\n",
    "    df = pd.read_csv('Frequencies.csv')\n",
    "\n",
    "    global_freq = int(open(\"global_frequency.txt\", \"r\").read())\n",
    "    print(global_freq)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    for i in df_doc_freq.iterrows():\n",
    "        local_freq = i[1]['Frequency']\n",
    "        doc_number = i[1]['Document']\n",
    "        doc_number = str(doc_number).zfill(3)\n",
    "        result = 1\n",
    "        for j in query.split():\n",
    "            entries = df.loc[df['Term'] == j]['Documents'].values[0]\n",
    "            entries = entries.split(', ',1)\n",
    "            entries[0] = entries[0].replace(\"[\", \"\")\n",
    "            entries[1] = entries[1].replace(']', '')\n",
    "            entries[1] = ast.literal_eval(entries[1])\n",
    "            if doc_number in entries[1].keys():\n",
    "                prob = (float(entries[1][doc_number])/float(local_freq)*lambd) + (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            else:\n",
    "                prob = (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            result *= prob\n",
    "        result_dict.append((doc_number, result))\n",
    "    result_sorted = sorted(result_dict, key=lambda x: x[1], reverse=True)\n",
    "    stop = timeit.default_timer()\n",
    "    return result_sorted[:limit], stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('031', 0.020922239783990253),\n",
       "  ('190', 0.010505573117323586),\n",
       "  ('301', 0.007441847627127508),\n",
       "  ('105', 0.006582412944163413),\n",
       "  ('070', 0.005243545625914651),\n",
       "  ('422', 0.004474871362937621),\n",
       "  ('466', 0.0039648754429049814),\n",
       "  ('061', 0.002720485398025341),\n",
       "  ('001', 8.890645065691988e-05),\n",
       "  ('002', 8.890645065691988e-05)],\n",
       " 0.8783825000000434)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model('1991', 10, 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TF IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0         Term  \\\n",
      "0              0       +bahia   \n",
      "1              1            -   \n",
      "2              2           --   \n",
      "3              3         .125   \n",
      "4              4        .3322   \n",
      "5              5            0   \n",
      "6              6         0.01   \n",
      "7              7          0.2   \n",
      "8              8         0.22   \n",
      "9              9          0.3   \n",
      "10            10         0.33   \n",
      "11            11         0.39   \n",
      "12            12          0.4   \n",
      "13            13         0.40   \n",
      "14            14          0.5   \n",
      "15            15         0.53   \n",
      "16            16         0.59   \n",
      "17            17          0.6   \n",
      "18            18         0.68   \n",
      "19            19          0.7   \n",
      "20            20         0.73   \n",
      "21            21         0.75   \n",
      "22            22         0.77   \n",
      "23            23          0.8   \n",
      "24            24         0.80   \n",
      "25            25          0.9   \n",
      "26            26         0.99   \n",
      "27            27  00:06:25.87   \n",
      "28            28  00:37:11.96   \n",
      "29            29  00:55:57.87   \n",
      "...          ...          ...   \n",
      "7371        7371          yen   \n",
      "7372        7372   yen/dollar   \n",
      "7373        7373       yergin   \n",
      "7374        7374    yesterday   \n",
      "7375        7375          yet   \n",
      "7376        7376      yeutter   \n",
      "7377        7377          yfc   \n",
      "7378        7378        yield   \n",
      "7379        7379          ynk   \n",
      "7380        7380         york   \n",
      "7381        7381     yorkshir   \n",
      "7382        7382       yotaro   \n",
      "7383        7383       youcef   \n",
      "7384        7384        young   \n",
      "7385        7385       yousfi   \n",
      "7386        7386           yr   \n",
      "7387        7387       yr-ago   \n",
      "7388        7388            z   \n",
      "7389        7389         zair   \n",
      "7390        7390      zambian   \n",
      "7391        7391      zealand   \n",
      "7392        7392   zellerbach   \n",
      "7393        7393         zero   \n",
      "7394        7394        ziana   \n",
      "7395        7395      ziegler   \n",
      "7396        7396      zimbabw   \n",
      "7397        7397   zimbabwean   \n",
      "7398        7398          zip   \n",
      "7399        7399         zoet   \n",
      "7400        7400         zone   \n",
      "\n",
      "                                              Documents  \n",
      "0                                               ['001']  \n",
      "1     ['005', '016', '026', '046', '059', '060', '09...  \n",
      "2     ['005', '019', '028', '043', '049', '094', '17...  \n",
      "3                                               ['010']  \n",
      "4                                               ['179']  \n",
      "5                                               ['484']  \n",
      "6                                               ['226']  \n",
      "7                                               ['433']  \n",
      "8                                               ['487']  \n",
      "9                                               ['425']  \n",
      "10                                              ['487']  \n",
      "11                                              ['001']  \n",
      "12                                       ['305', '425']  \n",
      "13                                              ['452']  \n",
      "14                                ['133', '145', '328']  \n",
      "15                                              ['094']  \n",
      "16                                              ['267']  \n",
      "17                         ['054', '133', '435', '483']  \n",
      "18                                              ['267']  \n",
      "19                                       ['054', '133']  \n",
      "20                                              ['267']  \n",
      "21                                ['022', '402', '496']  \n",
      "22                                              ['267']  \n",
      "23                                ['133', '425', '483']  \n",
      "24                                              ['452']  \n",
      "25                                              ['054']  \n",
      "26                                              ['005']  \n",
      "27                                              ['376']  \n",
      "28                                              ['377']  \n",
      "29                                              ['378']  \n",
      "...                                                 ...  \n",
      "7371  ['169', '207', '259', '376', '377', '435', '43...  \n",
      "7372                                     ['207', '448']  \n",
      "7373                                            ['292']  \n",
      "7374  ['058', '059', '080', '098', '107', '127', '13...  \n",
      "7375  ['004', '016', '107', '133', '142', '150', '15...  \n",
      "7376                                     ['218', '254']  \n",
      "7377                                            ['289']  \n",
      "7378  ['122', '239', '249', '265', '317', '359', '438']  \n",
      "7379                                            ['289']  \n",
      "7380  ['001', '026', '060', '072', '077', '078', '07...  \n",
      "7381                                            ['379']  \n",
      "7382                                            ['376']  \n",
      "7383                                            ['493']  \n",
      "7384                                            ['272']  \n",
      "7385                                            ['493']  \n",
      "7386                              ['024', '074', '140']  \n",
      "7387                                     ['238', '412']  \n",
      "7388                                            ['401']  \n",
      "7389                                            ['158']  \n",
      "7390                                            ['158']  \n",
      "7391                              ['048', '093', '176']  \n",
      "7392                                            ['185']  \n",
      "7393                       ['194', '246', '262', '265']  \n",
      "7394                                            ['288']  \n",
      "7395                                            ['469']  \n",
      "7396                                            ['288']  \n",
      "7397                                            ['288']  \n",
      "7398                                            ['265']  \n",
      "7399                                            ['399']  \n",
      "7400                                     ['001', '422']  \n",
      "\n",
      "[7401 rows x 3 columns]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_range' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d426f72972e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_range' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import ast\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "df_tfidf = pd.read_csv('Konstruksi Indeks.csv')\n",
    "doc = df_tfidf['Documents']\n",
    "term = df_tfidf['Term']\n",
    "df_tfidf\n",
    "print(df_tfidf)\n",
    "\n",
    "query = \"1991\"\n",
    "data = query.split()\n",
    "z=1\n",
    "count = 1\n",
    "N = 500+1\n",
    "\n",
    "dict_w = dict()\n",
    "dict_f = dict()\n",
    "dict_tf = dict()\n",
    "dict_idf = dict()\n",
    "dict_dj = dict()\n",
    "dict_sim = dict()\n",
    "\n",
    "for q in data:\n",
    "    for i in range (0,max_range):\n",
    "        ni = 0\n",
    "        start = 2\n",
    "        end = 5\n",
    "        count = 1\n",
    "        if (term[i] == q):\n",
    "            for y in range (0,int(len(doc[i])/7)):\n",
    "                if(int(len(doc[i])/7)!=1):\n",
    "                    if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "                        ni+=1\n",
    "                        dict_f['f',doc[i][start:end]]=count\n",
    "                    else:\n",
    "                        dict_f['f',doc[i][start:end]]=count\n",
    "                        count+=1\n",
    "                        ni=1\n",
    "                else:\n",
    "                    dict_f['f',doc[i][start:end]]=count\n",
    "                    ni+=1\n",
    "                    \n",
    "                start = start+7\n",
    "                end = end+7\n",
    "                \n",
    "            dict_idf[q] = math.log((1+N/ni),10)\n",
    "\n",
    "            \n",
    "for key in dict_f:\n",
    "    dict_tf['tf',z] = 1+math.log(dict_f[key],10)\n",
    "    z+=1\n",
    "\n",
    "z=1\n",
    "\n",
    "for key in dict_tf:\n",
    "    for key2 in dict_idf:\n",
    "        dict_w['w',z] = dict_tf[key]*dict_idf[key2]\n",
    "        z+=1\n",
    "\n",
    "z=1\n",
    "\n",
    "for key in dict_w:\n",
    "    dict_dj['dj',z] = math.sqrt(pow(dict_w[key],2))\n",
    "    z+=1\n",
    "\n",
    "z=1\n",
    "for key in dict_dj:\n",
    "    for key2 in dict_w:\n",
    "        dict_sim[\"d\",z] = dict_w[key2]/dict_dj[key]\n",
    "        z+=1\n",
    "        break\n",
    "        \n",
    "print(data)\n",
    "print(\"N : \",N)\n",
    "print(\"f : \",dict_f)\n",
    "print(\"Tf : \",dict_tf)\n",
    "print(\"IDF : \",dict_idf)\n",
    "print(\"W : \", dict_w)\n",
    "print(\"Panjang : \", dict_dj)\n",
    "print(\"Sim : \",dict_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
