{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Language Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import ast\n",
    "from itertools import islice\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def join_tags(tokens):\n",
    "    for i in enumerate(tokens):\n",
    "        hit = 0\n",
    "        index = i[0]\n",
    "        y = i[1]\n",
    "        width = len(y)\n",
    "        \n",
    "        for x in y:\n",
    "            if x is '-':\n",
    "#                 save = tokens[index][hit+1:]\n",
    "#                 tokens[index] = tokens[index][0:hit] + ''\n",
    "#                 tokens[index+1] = save\n",
    "#                 print(save)\n",
    "#                 print(tokens[index])\n",
    "                tokens[index] = tokens[index][0:hit] + ' ' + tokens[index][hit+1:]\n",
    "            hit += 1\n",
    "        \n",
    "        if tokens[index] is ' ':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index] == '  ':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if y[(width-1):width] is '.':\n",
    "            tokens[index] = y[0:(width-1)]\n",
    "        \n",
    "        if tokens[index][0:1] is '-' or tokens[index][1:2] is '-':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index][-2:-1] is \"'\":\n",
    "            tokens[index] = tokens[index][0:-2]\n",
    "        \n",
    "        if  tokens[index][:1] is \"'\" :\n",
    "            tokens[index] = tokens[index][1:]\n",
    "        \n",
    "        if tokens[index] is \"s\" or tokens[index] is \"re\":\n",
    "            tokens[index-1] = tokens[index-1]\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index] is '<' and tokens[index+2] is '>':\n",
    "            tokens[index+2] = '<'+ tokens[index+1]+'>'\n",
    "            tokens[index] =''\n",
    "            tokens[index+1] = ''\n",
    "            \n",
    "        if tokens[index] is '&' and tokens[index+2] is ';':\n",
    "            tokens[index+2] = ''\n",
    "            tokens[index] = ''\n",
    "            tokens[index+1] = ''\n",
    "            \n",
    "        if tokens[index] is '(' or tokens[index] is ')':\n",
    "            tokens[index] = ''\n",
    "            \n",
    "        if tokens[index][0:1] is ':':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index][0:1] is '.' and tokens[index][1:2] is '.':\n",
    "            tokens[index] = ''\n",
    "\n",
    "        if y[(width-2):(width-1)] == '.' and y[(width-1):(width)] == '0':\n",
    "            tokens[index] = y[0:(width-2)]\n",
    "        \n",
    "    tokens_clean = [x for x in tokens if x != '' and x not in \".,!?'\" and x not in '<>']\n",
    "        \n",
    "    return tokens_clean\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def guess_date(string):\n",
    "    for fmt in [\"%Y/%m/%d\", \"%d-%m-%Y\", \"%Y%m%d\", \"%d-%b-%y\", \"%d-%m-%y\", \"%d-%b-%Y\", \"%d %b %y\", \"%d %m %y\", \"%d %b %Y\"]:\n",
    "        try:\n",
    "            return str(datetime.datetime.strptime(string, fmt).date())\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return string\n",
    "\n",
    "def preprocess(text_process):\n",
    "    text_process = text_process.lower()\n",
    "    tokens = word_tokenize(text_process)\n",
    "    tokens_clean = join_tags(tokens)\n",
    "    tokens_clean = [guess_date(i) for i in tokens_clean]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [guess_date(w) for w in tokens_clean if not w in stop_words]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stemmed_words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    clean_text = lemmatized_text\n",
    "    clean_words = sorted(list([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stemmed_words]))\n",
    "    return clean_text, clean_words\n",
    "\n",
    "def build_index_language_model():\n",
    "    dict_term_doc = dict()\n",
    "    clean_words_list = list()\n",
    "    raw_words_list = list()\n",
    "    dict_term_doc_final = dict()\n",
    "    data = dict()\n",
    "    key = list()\n",
    "    value = list()\n",
    "    doc_freqs = dict()\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    for index, i in enumerate(os.listdir('Data')):\n",
    "        try:\n",
    "            filename = i\n",
    "            f = open('Data/' + filename, 'r')\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "\n",
    "            clean_text, clean_words = preprocess(text)\n",
    "            clean_words_list.extend(clean_words)\n",
    "\n",
    "    #         print(clean_words)\n",
    "            doc_freq = 0\n",
    "            for w in clean_words:\n",
    "                doc_freq+=1\n",
    "                doc_freqs[filename[4:7]] = doc_freq\n",
    "                if w not in dict_term_doc:\n",
    "                    dict_term_doc[w] = [1, {filename[4:7]: 1}]\n",
    "                else:\n",
    "                    if filename[4:7] in dict_term_doc[w][1]:\n",
    "                        dict_term_doc[w][1][filename[4:7]]+=1\n",
    "                    else:\n",
    "                        dict_term_doc[w][1][filename[4:7]]=1\n",
    "                    dict_term_doc[w][0]+=1\n",
    "    #         target = 'Clean/' + filename[:-3] +'xml'\n",
    "    #         new_file = open(target, 'w+')\n",
    "    #         new_file.write(clean_text)\n",
    "    #         new_file.close()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "#     print(doc_freqs)\n",
    "    df_doc_freqs = pd.DataFrame(doc_freqs.items(), columns = ['Document', 'Frequency'])\n",
    "    df_doc_freqs.to_csv('Document Frequencies.csv')\n",
    "\n",
    "    for i in sorted(dict_term_doc.keys()):\n",
    "        dict_term_doc_final[i] = dict_term_doc[i]\n",
    "    df_term_doc = pd.DataFrame(dict_term_doc_final.items(), columns = ['Term', 'Documents'])\n",
    "    df_term_doc.to_csv('Frequencies.csv')\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    return stop-start\n",
    "\n",
    "def language_model(query, limit, lambd):\n",
    "    clean_query_text, clean_query_words = preprocess(query)\n",
    "    print(clean_query_words)\n",
    "#     lambd = 1/4\n",
    "#     limit = 10\n",
    "    print(clean_query_text)\n",
    "    result_dict = list()\n",
    "\n",
    "    df_doc_freq = pd.read_csv('Document Frequencies.csv')\n",
    "    df = pd.read_csv('Frequencies.csv')\n",
    "\n",
    "    global_freq = int(open(\"global_frequency.txt\", \"r\").read())\n",
    "#     print(global_freq)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    for i in df_doc_freq.iterrows():\n",
    "        local_freq = i[1]['Frequency']\n",
    "        doc_number = i[1]['Document']\n",
    "        doc_number = str(doc_number).zfill(3)\n",
    "        result = 1\n",
    "        for j in clean_query_words:\n",
    "            entries = df.loc[df['Term'] == j]['Documents'].values[0]\n",
    "            entries = entries.split(', ',1)\n",
    "            entries[0] = entries[0].replace(\"[\", \"\")\n",
    "            entries[1] = entries[1].replace(']', '')\n",
    "            entries[1] = ast.literal_eval(entries[1])\n",
    "            if doc_number in entries[1].keys():\n",
    "                prob = (float(entries[1][doc_number])/float(local_freq)*lambd) + (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            else:\n",
    "                prob = (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            result *= prob\n",
    "        result_dict.append((doc_number, result))\n",
    "    result_sorted = sorted(result_dict, key=lambda x: x[1], reverse=True)\n",
    "    stop = timeit.default_timer()\n",
    "    return result_sorted[:limit], stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.93260250000094"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_index_language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chairman', 'close', 'last']\n",
      "chairman close last\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('322', 5.593024770203372e-08),\n",
       "  ('382', 4.481750615211067e-08),\n",
       "  ('107', 2.9617524346952403e-08),\n",
       "  ('446', 2.5298190784475663e-08),\n",
       "  ('153', 2.268158584271778e-08),\n",
       "  ('061', 1.734042541913407e-08),\n",
       "  ('231', 1.6530145952952898e-08),\n",
       "  ('438', 1.4461044377366107e-08),\n",
       "  ('269', 1.2719302552212365e-08),\n",
       "  ('399', 1.1475664930874336e-08),\n",
       "  ('150', 1.0939727305680492e-08),\n",
       "  ('052', 9.877104696502591e-09),\n",
       "  ('156', 9.674380447691085e-09),\n",
       "  ('404', 9.480015961717168e-09),\n",
       "  ('232', 9.164830308786491e-09),\n",
       "  ('194', 9.070927312262424e-09),\n",
       "  ('492', 8.64470516107788e-09),\n",
       "  ('098', 8.550965984857386e-09),\n",
       "  ('161', 8.539687777260021e-09),\n",
       "  ('419', 8.427877628552574e-09),\n",
       "  ('420', 7.189829770396591e-09),\n",
       "  ('430', 7.189829770396591e-09),\n",
       "  ('276', 7.053243736861799e-09),\n",
       "  ('255', 6.836511029195595e-09),\n",
       "  ('102', 6.599010460401655e-09),\n",
       "  ('494', 6.296344205947479e-09),\n",
       "  ('324', 5.569214409258104e-09),\n",
       "  ('018', 5.510502312904738e-09),\n",
       "  ('208', 5.416432950338065e-09),\n",
       "  ('154', 5.2672979887824e-09),\n",
       "  ('318', 5.064253461032365e-09),\n",
       "  ('188', 4.767342808650951e-09),\n",
       "  ('371', 4.588935969281417e-09),\n",
       "  ('263', 4.51215225474097e-09),\n",
       "  ('233', 4.0263289080186415e-09),\n",
       "  ('330', 3.601764185539703e-09),\n",
       "  ('195', 3.548670561350027e-09),\n",
       "  ('254', 3.4126053577219383e-09),\n",
       "  ('495', 3.297873346252573e-09),\n",
       "  ('227', 3.1164122786877683e-09),\n",
       "  ('488', 3.011697844390734e-09),\n",
       "  ('355', 2.9863400924050955e-09),\n",
       "  ('028', 2.9782577467294554e-09),\n",
       "  ('204', 2.69165632196146e-09),\n",
       "  ('133', 2.6203169917351118e-09),\n",
       "  ('392', 2.5981867864201973e-09),\n",
       "  ('409', 2.4754003135899824e-09),\n",
       "  ('111', 2.418319423618754e-09),\n",
       "  ('439', 2.3763074904768963e-09),\n",
       "  ('457', 2.3648054054379457e-09),\n",
       "  ('458', 2.2703666139805394e-09),\n",
       "  ('155', 2.2511354710765036e-09),\n",
       "  ('214', 2.2444350276212512e-09),\n",
       "  ('216', 2.2444350276212512e-09),\n",
       "  ('234', 2.224599940041796e-09),\n",
       "  ('069', 2.17048198504106e-09),\n",
       "  ('095', 2.1659221508672817e-09),\n",
       "  ('262', 2.1367944804694905e-09),\n",
       "  ('116', 2.1241499583643142e-09),\n",
       "  ('306', 2.1241499583643142e-09),\n",
       "  ('148', 2.0378765293800278e-09),\n",
       "  ('455', 2.017533646977483e-09),\n",
       "  ('026', 1.959187577669086e-09),\n",
       "  ('425', 1.9405844831069876e-09),\n",
       "  ('261', 1.8744683077922845e-09),\n",
       "  ('135', 1.789449209454805e-09),\n",
       "  ('441', 1.7254823649607515e-09),\n",
       "  ('224', 1.716996346577624e-09),\n",
       "  ('338', 1.613888032990148e-09),\n",
       "  ('378', 1.5455787752384451e-09),\n",
       "  ('375', 1.4740423084591027e-09),\n",
       "  ('043', 1.4644615316582981e-09),\n",
       "  ('169', 1.4644615316582981e-09),\n",
       "  ('284', 1.458157082675385e-09),\n",
       "  ('203', 1.4457421677551873e-09),\n",
       "  ('145', 1.392887493205227e-09),\n",
       "  ('328', 1.392887493205227e-09),\n",
       "  ('405', 1.3845284814150873e-09),\n",
       "  ('367', 1.3601679327695375e-09),\n",
       "  ('288', 1.3444994108707566e-09),\n",
       "  ('256', 1.329266125691386e-09),\n",
       "  ('316', 1.329266125691386e-09),\n",
       "  ('279', 1.3144501907909025e-09),\n",
       "  ('178', 1.3000346865634053e-09),\n",
       "  ('350', 1.3000346865634053e-09),\n",
       "  ('040', 1.2860035957819745e-09),\n",
       "  ('496', 1.2656447581775454e-09),\n",
       "  ('339', 1.259034745968315e-09),\n",
       "  ('444', 1.259034745968315e-09),\n",
       "  ('491', 1.1973681686504965e-09),\n",
       "  ('134', 1.1177872783688424e-09),\n",
       "  ('386', 1.1034452278125883e-09),\n",
       "  ('383', 1.1003220805503383e-09),\n",
       "  ('174', 1.0941436805315394e-09),\n",
       "  ('114', 1.0761343017533384e-09),\n",
       "  ('303', 1.0674139709765252e-09),\n",
       "  ('197', 1.0588753137575622e-09),\n",
       "  ('340', 1.0588753137575622e-09),\n",
       "  ('012', 1.0505127113266192e-09),\n",
       "  ('463', 1.0402988457621087e-09),\n",
       "  ('268', 1.0238412882910536e-09),\n",
       "  ('447', 9.96477434080526e-10),\n",
       "  ('498', 9.96477434080526e-10),\n",
       "  ('451', 9.665264518355488e-10),\n",
       "  ('180', 9.556351855646477e-10),\n",
       "  ('242', 9.556351855646477e-10),\n",
       "  ('235', 8.886292469844141e-10),\n",
       "  ('484', 8.757073443830365e-10),\n",
       "  ('267', 8.706833086516209e-10),\n",
       "  ('060', 8.608725932075968e-10),\n",
       "  ('047', 8.513660859943952e-10),\n",
       "  ('138', 7.922661915011347e-10),\n",
       "  ('343', 7.884845018237385e-10),\n",
       "  ('259', 7.810765343734967e-10),\n",
       "  ('265', 7.490283318419788e-10),\n",
       "  ('230', 7.46885915372381e-10),\n",
       "  ('314', 7.044333989938695e-10),\n",
       "  ('246', 7.004502839848191e-10),\n",
       "  ('136', 6.827450781624667e-10),\n",
       "  ('372', 6.491077946413551e-10),\n",
       "  ('207', 6.322586993730848e-10),\n",
       "  ('454', 6.115933164464706e-10),\n",
       "  ('078', 5.915110626666483e-10),\n",
       "  ('448', 5.848135006268323e-10),\n",
       "  ('201', 5.642579748542262e-10),\n",
       "  ('292', 5.238466858885067e-10),\n",
       "  ('445', 4.949173345813013e-10),\n",
       "  ('001', 4.829673910586326e-10),\n",
       "  ('376', 4.072777833596591e-10)],\n",
       " 2.6029521000000386)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model('Chairman closed the last', 129, 1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TF IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import ast\n",
    "from itertools import islice\n",
    "import math\n",
    "\n",
    "def build_index_tfidf():\n",
    "    df_tfidf = pd.read_csv('Konstruksi Indeks.csv')\n",
    "    doc = df_tfidf['Documents']\n",
    "    term = df_tfidf['Term']\n",
    "    \n",
    "    \n",
    "    dict_tfidf = dict()\n",
    "    dict_doc = dict()\n",
    "    list_doc = list()\n",
    "    dic = dict()\n",
    "    max_range = len(term)\n",
    "    N = 500\n",
    "    for i in range (0,max_range):\n",
    "        start = 2\n",
    "        end = 5\n",
    "        count = 1\n",
    "        for y in range (0,int(len(doc[i])/7)):\n",
    "            if(int(len(doc[i])/7)!=1):\n",
    "                if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "                    dict_doc[term[i]+\" \"+doc[i][start:end]] = count\n",
    "                    dic[doc[i][start:end]] = count\n",
    "                else:\n",
    "                    dict_doc[term[i]+\" \"+doc[i][start:end]] = count\n",
    "                    dic[doc[i][start:end]] = count\n",
    "                    count+=1\n",
    "            else :\n",
    "                dict_doc[term[i]+\" \"+doc[i][start:end]] = count\n",
    "                dic[doc[i][start:end]] = count\n",
    "            start = start+7\n",
    "            end = end+7\n",
    "            dict_tfidf[term[i]] = count\n",
    "\n",
    "def tfidf(query):\n",
    "    text, data = preprocess(query)\n",
    "    z=1\n",
    "    count = 1\n",
    "    N = 500+1\n",
    "\n",
    "    dict_w = dict()\n",
    "    dict_f = dict()\n",
    "    dict_tf = dict()\n",
    "    dict_idf = dict()\n",
    "    dict_dj = dict()\n",
    "    dict_sim = dict()\n",
    "\n",
    "    for q in data:\n",
    "        for i in range (0,max_range):\n",
    "            ni = 0\n",
    "            start = 2\n",
    "            end = 5\n",
    "            count = 1\n",
    "            if (term[i] == q):\n",
    "                for y in range (0,int(len(doc[i])/7)):\n",
    "                    if(int(len(doc[i])/7)!=1):\n",
    "                        if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "                            ni+=1\n",
    "                            dict_f['f',doc[i][start:end]]=count\n",
    "                        else:\n",
    "                            dict_f['f',doc[i][start:end]]=count\n",
    "                            count+=1\n",
    "                            ni=1\n",
    "                    else:\n",
    "                        dict_f['f',doc[i][start:end]]=count\n",
    "                        ni+=1\n",
    "\n",
    "                    start = start+7\n",
    "                    end = end+7\n",
    "\n",
    "                dict_idf[q] = math.log((1+N/ni),10)\n",
    "\n",
    "\n",
    "    for key in dict_f:\n",
    "        dict_tf['tf',z] = 1+math.log(dict_f[key],10)\n",
    "        z+=1\n",
    "\n",
    "    z=1\n",
    "\n",
    "    for key in dict_tf:\n",
    "        for key2 in dict_idf:\n",
    "            dict_w['w',z] = dict_tf[key]*dict_idf[key2]\n",
    "            z+=1\n",
    "\n",
    "    z=1\n",
    "\n",
    "    for key in dict_w:\n",
    "        dict_dj['dj',z] = math.sqrt(pow(dict_w[key],2))\n",
    "        z+=1\n",
    "\n",
    "    z=1\n",
    "    for key in dict_dj:\n",
    "        for key2 in dict_w:\n",
    "            dict_sim[\"d\",z] = dict_w[key2]/dict_dj[key]\n",
    "            z+=1\n",
    "            break\n",
    "    stop = timeit.default_timer()\n",
    "#     print(data)\n",
    "#     print(\"N : \",N)\n",
    "#     print(\"f : \",dict_f)\n",
    "#     print(\"Tf : \",dict_tf)\n",
    "#     print(\"IDF : \",dict_idf)\n",
    "#     print(\"W : \", dict_w)\n",
    "#     print(\"Panjang : \", dict_dj)\n",
    "#     print(\"Sim : \",dict_sim)\n",
    "    \n",
    "    result = [(i[1], j)for i,j in dict_f.items()]\n",
    "    \n",
    "    return result, stop-start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_index_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('031', 1),\n",
       "  ('061', 1),\n",
       "  ('070', 1),\n",
       "  ('105', 1),\n",
       "  ('190', 1),\n",
       "  ('301', 1),\n",
       "  ('422', 1),\n",
       "  ('466', 1)],\n",
       " 4171.7445843)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(\"1991\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
