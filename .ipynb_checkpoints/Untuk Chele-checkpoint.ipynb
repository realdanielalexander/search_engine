{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Language Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import ast\n",
    "from itertools import islice\n",
    "import datetime\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def join_tags(tokens):\n",
    "    for i in enumerate(tokens):\n",
    "        hit = 0\n",
    "        index = i[0]\n",
    "        y = i[1]\n",
    "        width = len(y)\n",
    "        \n",
    "        for x in y:\n",
    "            if x is '-':\n",
    "#                 save = tokens[index][hit+1:]\n",
    "#                 tokens[index] = tokens[index][0:hit] + ''\n",
    "#                 tokens[index+1] = save\n",
    "#                 print(save)\n",
    "#                 print(tokens[index])\n",
    "                tokens[index] = tokens[index][0:hit] + ' ' + tokens[index][hit+1:]\n",
    "            hit += 1\n",
    "        \n",
    "        if tokens[index] is ' ':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index] == '  ':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if y[(width-1):width] is '.':\n",
    "            tokens[index] = y[0:(width-1)]\n",
    "        \n",
    "        if tokens[index][0:1] is '-' or tokens[index][1:2] is '-':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index][-2:-1] is \"'\":\n",
    "            tokens[index] = tokens[index][0:-2]\n",
    "        \n",
    "        if  tokens[index][:1] is \"'\" :\n",
    "            tokens[index] = tokens[index][1:]\n",
    "        \n",
    "        if tokens[index] is \"s\" or tokens[index] is \"re\":\n",
    "            tokens[index-1] = tokens[index-1]\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index] is '<' and tokens[index+2] is '>':\n",
    "            tokens[index+2] = '<'+ tokens[index+1]+'>'\n",
    "            tokens[index] =''\n",
    "            tokens[index+1] = ''\n",
    "            \n",
    "        if tokens[index] is '&' and tokens[index+2] is ';':\n",
    "            tokens[index+2] = ''\n",
    "            tokens[index] = ''\n",
    "            tokens[index+1] = ''\n",
    "            \n",
    "        if tokens[index] is '(' or tokens[index] is ')':\n",
    "            tokens[index] = ''\n",
    "            \n",
    "        if tokens[index][0:1] is ':':\n",
    "            tokens[index] = ''\n",
    "        \n",
    "        if tokens[index][0:1] is '.' and tokens[index][1:2] is '.':\n",
    "            tokens[index] = ''\n",
    "\n",
    "        if y[(width-2):(width-1)] == '.' and y[(width-1):(width)] == '0':\n",
    "            tokens[index] = y[0:(width-2)]\n",
    "        \n",
    "    tokens_clean = [x for x in tokens if x != '' and x not in \".,!?'\" and x not in '<>']\n",
    "        \n",
    "    return tokens_clean\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def guess_date(string):\n",
    "    for fmt in [\"%Y/%m/%d\", \"%d-%m-%Y\", \"%Y%m%d\", \"%d-%b-%y\", \"%d-%m-%y\", \"%d-%b-%Y\", \"%d %b %y\", \"%d %m %y\", \"%d %b %Y\"]:\n",
    "        try:\n",
    "            return str(datetime.datetime.strptime(string, fmt).date())\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return string\n",
    "\n",
    "def preprocess(text_process):\n",
    "    text_process = text_process.lower()\n",
    "    tokens = word_tokenize(text_process)\n",
    "    tokens_clean = join_tags(tokens)\n",
    "    tokens_clean = [guess_date(i) for i in tokens_clean]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [guess_date(w) for w in tokens_clean if not w in stop_words]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stemmed_words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    clean_text = lemmatized_text\n",
    "    clean_words = sorted(list([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in stemmed_words]))\n",
    "    return clean_text, clean_words\n",
    "\n",
    "def build_index_language_model():\n",
    "    dict_term_doc = dict()\n",
    "    clean_words_list = list()\n",
    "    raw_words_list = list()\n",
    "    dict_term_doc_final = dict()\n",
    "    data = dict()\n",
    "    key = list()\n",
    "    value = list()\n",
    "    doc_freqs = dict()\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    for index, i in enumerate(os.listdir('Data')):\n",
    "        try:\n",
    "            filename = i\n",
    "            f = open('Data/' + filename, 'r')\n",
    "            text = f.read()\n",
    "            f.close()\n",
    "\n",
    "            clean_text, clean_words = preprocess(text)\n",
    "            clean_words_list.extend(clean_words)\n",
    "\n",
    "    #         print(clean_words)\n",
    "            doc_freq = 0\n",
    "            for w in clean_words:\n",
    "                doc_freq+=1\n",
    "                doc_freqs[filename[4:7]] = doc_freq\n",
    "                if w not in dict_term_doc:\n",
    "                    dict_term_doc[w] = [1, {filename[4:7]: 1}]\n",
    "                else:\n",
    "                    if filename[4:7] in dict_term_doc[w][1]:\n",
    "                        dict_term_doc[w][1][filename[4:7]]+=1\n",
    "                    else:\n",
    "                        dict_term_doc[w][1][filename[4:7]]=1\n",
    "                    dict_term_doc[w][0]+=1\n",
    "    #         target = 'Clean/' + filename[:-3] +'xml'\n",
    "    #         new_file = open(target, 'w+')\n",
    "    #         new_file.write(clean_text)\n",
    "    #         new_file.close()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "#     print(doc_freqs)\n",
    "    df_doc_freqs = pd.DataFrame(doc_freqs.items(), columns = ['Document', 'Frequency'])\n",
    "    df_doc_freqs.to_csv('Document Frequencies.csv')\n",
    "\n",
    "    for i in sorted(dict_term_doc.keys()):\n",
    "        dict_term_doc_final[i] = dict_term_doc[i]\n",
    "    df_term_doc = pd.DataFrame(dict_term_doc_final.items(), columns = ['Term', 'Documents'])\n",
    "    df_term_doc.to_csv('Frequencies.csv')\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    return stop-start\n",
    "\n",
    "def language_model(query, limit, lambd):\n",
    "    clean_query_text, clean_query_words = preprocess(query)\n",
    "    print(clean_query_words)\n",
    "#     lambd = 1/4\n",
    "#     limit = 10\n",
    "    print(clean_query_text)\n",
    "    result_dict = list()\n",
    "\n",
    "    df_doc_freq = pd.read_csv('Document Frequencies.csv')\n",
    "    df = pd.read_csv('Frequencies.csv')\n",
    "\n",
    "    global_freq = int(open(\"global_frequency.txt\", \"r\").read())\n",
    "#     print(global_freq)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    for i in df_doc_freq.iterrows():\n",
    "        local_freq = i[1]['Frequency']\n",
    "        doc_number = i[1]['Document']\n",
    "        doc_number = str(doc_number).zfill(3)\n",
    "        result = 1\n",
    "        for j in clean_query_words:\n",
    "            entries = df.loc[df['Term'] == j]['Documents'].values[0]\n",
    "            entries = entries.split(', ',1)\n",
    "            entries[0] = entries[0].replace(\"[\", \"\")\n",
    "            entries[1] = entries[1].replace(']', '')\n",
    "            entries[1] = ast.literal_eval(entries[1])\n",
    "            if doc_number in entries[1].keys():\n",
    "                prob = (float(entries[1][doc_number])/float(local_freq)*lambd) + (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            else:\n",
    "                prob = (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            result *= prob\n",
    "        result_dict.append((doc_number, result))\n",
    "    result_sorted = sorted(result_dict, key=lambda x: x[1], reverse=True)\n",
    "    stop = timeit.default_timer()\n",
    "    return result_sorted[:limit], stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.26878349999993"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_index_language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1987-03-09', 'access']\n",
      "access 1987-03-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('387', 4.788025715046517e-05),\n",
       "  ('449', 4.788025715046517e-05),\n",
       "  ('032', 2.478825242639475e-06),\n",
       "  ('055', 2.478825242639475e-06),\n",
       "  ('375', 2.4596880516608156e-06),\n",
       "  ('265', 1.941595110250042e-06),\n",
       "  ('144', 1.766401034860307e-06),\n",
       "  ('134', 1.7592027148330282e-06),\n",
       "  ('384', 1.568170375647554e-06),\n",
       "  ('321', 1.4109529562719219e-06),\n",
       "  ('443', 1.3230464422124287e-06),\n",
       "  ('348', 1.0900119544739339e-06),\n",
       "  ('146', 1.0362514400933324e-06),\n",
       "  ('327', 9.074015116050426e-07),\n",
       "  ('450', 9.074015116050426e-07),\n",
       "  ('442', 8.891249260038669e-07),\n",
       "  ('390', 8.54809704058802e-07),\n",
       "  ('143', 8.231858720702127e-07),\n",
       "  ('326', 8.082861819986659e-07),\n",
       "  ('141', 6.961721206406334e-07),\n",
       "  ('324', 6.857242152811034e-07),\n",
       "  ('381', 6.857242152811034e-07),\n",
       "  ('203', 6.563093432688884e-07),\n",
       "  ('145', 6.294899011401042e-07),\n",
       "  ('206', 6.294899011401042e-07),\n",
       "  ('328', 6.294899011401042e-07),\n",
       "  ('388', 6.210683371866211e-07),\n",
       "  ('149', 6.128873893460948e-07),\n",
       "  ('263', 6.128873893460948e-07),\n",
       "  ('110', 5.972072393184192e-07),\n",
       "  ('446', 5.89689359168164e-07),\n",
       "  ('205', 5.683227524253329e-07),\n",
       "  ('270', 5.683227524253329e-07),\n",
       "  ('385', 5.683227524253329e-07),\n",
       "  ('382', 5.61570534710066e-07),\n",
       "  ('444', 5.61570534710066e-07),\n",
       "  ('329', 5.5499145078237e-07),\n",
       "  ('116', 5.244740132864307e-07),\n",
       "  ('323', 5.078668496258498e-07),\n",
       "  ('148', 5.02585644323826e-07),\n",
       "  ('147', 4.923792700322743e-07),\n",
       "  ('330', 4.923792700322743e-07),\n",
       "  ('386', 4.826215275777141e-07),\n",
       "  ('389', 4.779017499774104e-07),\n",
       "  ('447', 4.28344085174222e-07),\n",
       "  ('325', 4.0761997080197957e-07),\n",
       "  ('209', 3.749637905790522e-07),\n",
       "  ('142', 3.6451381290771543e-07),\n",
       "  ('267', 3.6451381290771543e-07),\n",
       "  ('204', 3.4109812219972007e-07),\n",
       "  ('202', 3.3064468884793643e-07),\n",
       "  ('210', 3.266694395451454e-07),\n",
       "  ('266', 3.2280461383409866e-07),\n",
       "  ('264', 2.893408790189376e-07),\n",
       "  ('150', 2.703906291910262e-07),\n",
       "  ('322', 2.4696469221782914e-07),\n",
       "  ('441', 2.4401574117028384e-07),\n",
       "  ('269', 2.2965876848621002e-07),\n",
       "  ('201', 2.090279154625511e-07),\n",
       "  ('383', 1.926135871444575e-07),\n",
       "  ('268', 1.7967766300008425e-07),\n",
       "  ('445', 1.7384318857118174e-07),\n",
       "  ('261', 1.5606950352618723e-07),\n",
       "  ('262', 1.5490866778304534e-07),\n",
       "  ('208', 1.5433758639198225e-07),\n",
       "  ('207', 1.459673662997587e-07),\n",
       "  ('448', 1.3393007940982602e-07),\n",
       "  ('001', 4.840198834977802e-08),\n",
       "  ('002', 4.840198834977802e-08),\n",
       "  ('003', 4.840198834977802e-08),\n",
       "  ('004', 4.840198834977802e-08),\n",
       "  ('005', 4.840198834977802e-08),\n",
       "  ('006', 4.840198834977802e-08),\n",
       "  ('007', 4.840198834977802e-08),\n",
       "  ('008', 4.840198834977802e-08),\n",
       "  ('009', 4.840198834977802e-08),\n",
       "  ('010', 4.840198834977802e-08),\n",
       "  ('011', 4.840198834977802e-08),\n",
       "  ('012', 4.840198834977802e-08),\n",
       "  ('013', 4.840198834977802e-08),\n",
       "  ('014', 4.840198834977802e-08),\n",
       "  ('015', 4.840198834977802e-08),\n",
       "  ('016', 4.840198834977802e-08),\n",
       "  ('017', 4.840198834977802e-08),\n",
       "  ('018', 4.840198834977802e-08),\n",
       "  ('019', 4.840198834977802e-08),\n",
       "  ('020', 4.840198834977802e-08),\n",
       "  ('021', 4.840198834977802e-08),\n",
       "  ('022', 4.840198834977802e-08),\n",
       "  ('023', 4.840198834977802e-08),\n",
       "  ('024', 4.840198834977802e-08),\n",
       "  ('025', 4.840198834977802e-08),\n",
       "  ('026', 4.840198834977802e-08),\n",
       "  ('027', 4.840198834977802e-08),\n",
       "  ('028', 4.840198834977802e-08),\n",
       "  ('029', 4.840198834977802e-08),\n",
       "  ('030', 4.840198834977802e-08),\n",
       "  ('031', 4.840198834977802e-08),\n",
       "  ('033', 4.840198834977802e-08),\n",
       "  ('034', 4.840198834977802e-08),\n",
       "  ('035', 4.840198834977802e-08),\n",
       "  ('036', 4.840198834977802e-08),\n",
       "  ('037', 4.840198834977802e-08),\n",
       "  ('038', 4.840198834977802e-08),\n",
       "  ('039', 4.840198834977802e-08),\n",
       "  ('040', 4.840198834977802e-08),\n",
       "  ('041', 4.840198834977802e-08),\n",
       "  ('042', 4.840198834977802e-08),\n",
       "  ('043', 4.840198834977802e-08),\n",
       "  ('044', 4.840198834977802e-08),\n",
       "  ('045', 4.840198834977802e-08),\n",
       "  ('046', 4.840198834977802e-08),\n",
       "  ('047', 4.840198834977802e-08),\n",
       "  ('048', 4.840198834977802e-08),\n",
       "  ('049', 4.840198834977802e-08),\n",
       "  ('050', 4.840198834977802e-08),\n",
       "  ('051', 4.840198834977802e-08),\n",
       "  ('052', 4.840198834977802e-08),\n",
       "  ('053', 4.840198834977802e-08),\n",
       "  ('054', 4.840198834977802e-08),\n",
       "  ('056', 4.840198834977802e-08),\n",
       "  ('057', 4.840198834977802e-08),\n",
       "  ('058', 4.840198834977802e-08),\n",
       "  ('059', 4.840198834977802e-08),\n",
       "  ('060', 4.840198834977802e-08),\n",
       "  ('061', 4.840198834977802e-08),\n",
       "  ('062', 4.840198834977802e-08),\n",
       "  ('063', 4.840198834977802e-08),\n",
       "  ('064', 4.840198834977802e-08)],\n",
       " 1.5073593000001893)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model('accessed 9-Mar-87', 129, 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TF IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nltk\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# import string\n",
    "# import ast\n",
    "# from itertools import islice\n",
    "# import math\n",
    "\n",
    "# def build_index_tfidf():\n",
    "#     df_tfidf = pd.read_csv('Konstruksi Indeks.csv')\n",
    "#     doc = df_tfidf['Documents']\n",
    "#     term = df_tfidf['Term']\n",
    "    \n",
    "    \n",
    "#     dict_tfidf = dict()\n",
    "#     dict_doc = dict()\n",
    "#     list_doc = list()\n",
    "#     dic = dict()\n",
    "#     max_range = len(term)\n",
    "#     N = 500\n",
    "#     for i in range (0,max_range):\n",
    "#         start = 2\n",
    "#         end = 5\n",
    "#         count = 1\n",
    "#         for y in range (0,int(len(doc[i])/7)):\n",
    "#             if(int(len(doc[i])/7)!=1):\n",
    "#                 if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "#                     dict_doc[term[i]+\" \"+doc[i][start:end]] = count\n",
    "#                     dic[doc[i][start:end]] = count\n",
    "#                 else:\n",
    "#                     dict_doc[term[i]+\" \"+doc[i][start:end]] = count\n",
    "#                     dic[doc[i][start:end]] = count\n",
    "#                     count+=1\n",
    "#             else :\n",
    "#                 dict_doc[term[i]+\" \"+doc[i][start:end]] = count\n",
    "#                 dic[doc[i][start:end]] = count\n",
    "#             start = start+7\n",
    "#             end = end+7\n",
    "#             dict_tfidf[term[i]] = count\n",
    "\n",
    "# def tfidf(query):\n",
    "#     text, data = preprocess(query)\n",
    "#     z=1\n",
    "#     count = 1\n",
    "#     N = 500+1\n",
    "\n",
    "#     dict_w = dict()\n",
    "#     dict_f = dict()\n",
    "#     dict_tf = dict()\n",
    "#     dict_idf = dict()\n",
    "#     dict_dj = dict()\n",
    "#     dict_sim = dict()\n",
    "\n",
    "#     for q in data:\n",
    "#         for i in range (0,max_range):\n",
    "#             ni = 0\n",
    "#             start = 2\n",
    "#             end = 5\n",
    "#             count = 1\n",
    "#             if (term[i] == q):\n",
    "#                 for y in range (0,int(len(doc[i])/7)):\n",
    "#                     if(int(len(doc[i])/7)!=1):\n",
    "#                         if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "#                             ni+=1\n",
    "#                             dict_f['f',doc[i][start:end]]=count\n",
    "#                         else:\n",
    "#                             dict_f['f',doc[i][start:end]]=count\n",
    "#                             count+=1\n",
    "#                             ni=1\n",
    "#                     else:\n",
    "#                         dict_f['f',doc[i][start:end]]=count\n",
    "#                         ni+=1\n",
    "\n",
    "#                     start = start+7\n",
    "#                     end = end+7\n",
    "\n",
    "#                 dict_idf[q] = math.log((1+N/ni),10)\n",
    "\n",
    "\n",
    "#     for key in dict_f:\n",
    "#         dict_tf['tf',z] = 1+math.log(dict_f[key],10)\n",
    "#         z+=1\n",
    "\n",
    "#     z=1\n",
    "\n",
    "#     for key in dict_tf:\n",
    "#         for key2 in dict_idf:\n",
    "#             dict_w['w',z] = dict_tf[key]*dict_idf[key2]\n",
    "#             z+=1\n",
    "\n",
    "#     z=1\n",
    "\n",
    "#     for key in dict_w:\n",
    "#         dict_dj['dj',z] = math.sqrt(pow(dict_w[key],2))\n",
    "#         z+=1\n",
    "\n",
    "#     z=1\n",
    "#     for key in dict_dj:\n",
    "#         for key2 in dict_w:\n",
    "#             dict_sim[\"d\",z] = dict_w[key2]/dict_dj[key]\n",
    "#             z+=1\n",
    "#             break\n",
    "#     stop = timeit.default_timer()\n",
    "# #     print(data)\n",
    "# #     print(\"N : \",N)\n",
    "# #     print(\"f : \",dict_f)\n",
    "# #     print(\"Tf : \",dict_tf)\n",
    "# #     print(\"IDF : \",dict_idf)\n",
    "# #     print(\"W : \", dict_w)\n",
    "# #     print(\"Panjang : \", dict_dj)\n",
    "# #     print(\"Sim : \",dict_sim)\n",
    "    \n",
    "#     result = [(i[1], j)for i,j in dict_f.items()]\n",
    "    \n",
    "#     return result, stop-start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_index_tf_idf():\n",
    "#     dict_term_doc = dict()\n",
    "#     clean_words_list = list()\n",
    "#     raw_words_list = list()\n",
    "#     dict_term_doc_final = dict()\n",
    "#     data = dict()\n",
    "#     key = list()\n",
    "#     value = list()\n",
    "    \n",
    "#     start = timeit.default_timer()\n",
    "#     for i in os.listdir('Data'):\n",
    "#         try:\n",
    "#             filename = i\n",
    "#             f = open('Data/' + filename, 'r')\n",
    "#             text = f.read()\n",
    "#             f.close()\n",
    "\n",
    "#             clean_text, clean_words = preprocess(text)\n",
    "#             clean_words_list.extend(clean_words)\n",
    "#             for w in clean_words:\n",
    "#                 if w not in dict_term_doc:\n",
    "#                     dict_term_doc[w] = [filename[4:7]]\n",
    "#                 else:\n",
    "#                     dict_term_doc[w].append(filename[4:7])\n",
    "#     #         target = 'Clean/' + filename[:-3] +'xml'\n",
    "#     #         new_file = open(target, 'w+')\n",
    "#     #         new_file.write(clean_text)\n",
    "#     #         new_file.close()\n",
    "#         except Exception as e:\n",
    "#             print(str(e))\n",
    "\n",
    "#     for i in sorted(dict_term_doc.keys()):\n",
    "#         dict_term_doc_final[i] = dict_term_doc[i]\n",
    "\n",
    "#     df_term_doc = pd.DataFrame(dict_term_doc_final.items(), columns = ['Term', 'Documents'])\n",
    "\n",
    "#     df_term_doc.to_csv('Konstruksi Indeks.csv')\n",
    "#     stop = timeit.default_timer()\n",
    "    \n",
    "#     return stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import ast\n",
    "from itertools import islice\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "\n",
    "def cobain(kueri):\n",
    "    df_tfidf = pd.read_csv('Konstruksi Indeks.csv')\n",
    "    doc = df_tfidf['Documents']\n",
    "    term = df_tfidf['Term']\n",
    "    df_tfidf\n",
    "    max_range = len(term)\n",
    "\n",
    "    query = kueri\n",
    "    text, data = preprocess(query)\n",
    "    \n",
    "    z=1\n",
    "    count = 1\n",
    "    N = 500\n",
    "    plus = 0\n",
    "    list_angka = list()\n",
    "    list_nama = list()\n",
    "    list_final = list()\n",
    "    list_finalnama = list()\n",
    "\n",
    "    list_wnama = list()\n",
    "    list_wangka = list()\n",
    "    list_simnama = list()\n",
    "    list_simdoc = list()\n",
    "    list_simq = list()\n",
    "    list_simqnama = list()\n",
    "    list_sim = list()\n",
    "    list_finalsim = list()\n",
    "    list_finalsim2 = list()\n",
    "    list_finalsim2doc = list()\n",
    "\n",
    "    dict_w = dict()\n",
    "    dict_f = dict()\n",
    "    dict_tf = dict()\n",
    "    dict_idf = dict()\n",
    "    dict_dj = dict()\n",
    "    dict_dj2 = dict()\n",
    "    dict_sim = dict()\n",
    "\n",
    "    list_rank = list()\n",
    "    counter = 0\n",
    "\n",
    "    starts = timeit.default_timer()\n",
    "    \n",
    "    for q in data:\n",
    "        if data[counter]==data[counter-1]:\n",
    "            plus+=1\n",
    "            dict_f[q] = plus\n",
    "        else:\n",
    "            plus = 1\n",
    "            dict_f[q] = plus\n",
    "        counter+=1\n",
    "\n",
    "    for q in data:\n",
    "        for i in range (0,max_range):\n",
    "            ni = dict_f[q]\n",
    "            start = 2\n",
    "            end = 5\n",
    "            count = 1\n",
    "            if (term[i] == q):\n",
    "                for y in range (0,int(len(doc[i])/7)):\n",
    "                    if(int(len(doc[i])/7)!=1):\n",
    "                        if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "                            ni+=1\n",
    "                            dict_f[q +' doc ' + doc[i][start:end]]=count\n",
    "                        else:\n",
    "                            dict_f[q + ' doc' + doc[i][start:end]]=count\n",
    "                            count+=1\n",
    "                            ni=1\n",
    "                    else:\n",
    "                        dict_f[q +' doc ' + doc[i][start:end]]=count\n",
    "                        ni+=1\n",
    "\n",
    "                    start = start+7\n",
    "                    end = end+7\n",
    "\n",
    "                dict_idf[q] = math.log((1+N/ni),10)\n",
    "\n",
    "    for key in dict_f:\n",
    "        dict_tf[key] = 1+math.log(dict_f[key],10)\n",
    "\n",
    "\n",
    "    for key in dict_tf:\n",
    "        for key2 in dict_idf:\n",
    "            if key[0:len(key)-8] == key2:\n",
    "                dict_w[key] = dict_tf[key]*dict_idf[key2]\n",
    "            elif key == key2:\n",
    "                dict_w[key] = dict_tf[key]*dict_idf[key2]\n",
    "\n",
    "    for key in dict_w:\n",
    "        if(key[len(key)-7:len(key)-4] == 'doc' ):\n",
    "            dict_dj[key[len(key)-7:len(key)]] = (pow(dict_w[key],2))\n",
    "            list_angka.append(dict_dj[key[len(key)-7:len(key)]])\n",
    "            list_nama.append(key[len(key)-7:len(key)])\n",
    "        else:\n",
    "            dict_dj[key] = pow(dict_w[key],2)\n",
    "            list_nama.append('q')\n",
    "            list_angka.append(dict_dj[key])\n",
    "\n",
    "    for i in range(0,len(list_nama)):\n",
    "        for x in range(i+1,len(list_nama)):\n",
    "            if (list_nama[i]==list_nama[x]):\n",
    "\n",
    "                list_final.append(math.sqrt(list_angka[i]+list_angka[x]))\n",
    "                list_finalnama.append(list_nama[i])\n",
    "                list_angka[i]=0\n",
    "                list_angka[x] = 0\n",
    "\n",
    "    for i in range(0,len(list_nama)):\n",
    "        if list_angka[i]!=0:\n",
    "            list_final.append(math.sqrt(list_angka[i]))\n",
    "            list_finalnama.append(list_nama[i])\n",
    "\n",
    "    for i in range(0,len(list_final)):\n",
    "        dict_dj2[list_finalnama[i]] = list_final[i]\n",
    "\n",
    "\n",
    "    for key in dict_w:\n",
    "        list_wnama.append(key)\n",
    "        list_wangka.append(dict_w[key])\n",
    "\n",
    "    for i in range(0,len(list_wnama)):\n",
    "        for x in range(i+1,len(list_wnama)):\n",
    "            if(list_wnama[i]==list_wnama[x][0:len(list_wnama[x])-8]):\n",
    "                list_simnama.append(list_wnama[x][0:len(list_wnama[x])-8])\n",
    "                list_simdoc.append(list_wnama[x][len(list_wnama[x])-7:])\n",
    "                list_sim.append(list_wangka[x])\n",
    "                list_wangka[x] = 0\n",
    "\n",
    "    for i in range(0,len(list_wangka)):\n",
    "        if(list_wangka[i]!=0):\n",
    "            list_simq.append(list_wangka[i])\n",
    "            list_simqnama.append(list_wnama[i])\n",
    "\n",
    "    for i in range(0,len(list_sim)):\n",
    "        for y in range(0,len(list_simq)):\n",
    "            if(list_simnama[i]==list_simqnama[y]):\n",
    "                list_finalsim.append(list_sim[i]*list_simq[y])\n",
    "\n",
    "    for i in range(0,len(list_finalsim)):\n",
    "        for x in range(i+1,len(list_simdoc)):\n",
    "            if list_simdoc[i]==list_simdoc[x]:\n",
    "                list_finalsim2.append(list_finalsim[i]+list_finalsim[x])\n",
    "                list_finalsim[i] = 0\n",
    "                list_finalsim[x] = 0\n",
    "                list_finalsim2doc.append(list_simdoc[i])\n",
    "\n",
    "    for i in range(0,len(list_finalsim)):\n",
    "        if(list_finalsim[i]!=0):\n",
    "            list_finalsim2.append(list_finalsim[i])\n",
    "            list_finalsim2doc.append(list_simdoc[i])\n",
    "\n",
    "    for i in range(0,len(list_finalsim2)):\n",
    "        try:\n",
    "            dict_sim[list_finalsim2doc[i]] = list_finalsim2[i]/(dict_dj2[list_finalsim2doc[i]]*dict_dj2['q'])\n",
    "        except:\n",
    "            print(\"0\")\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    return dict_sim, stop-start\n",
    "\n",
    "\n",
    "# sorted(cobain('doubt zone').items())\n",
    "\n",
    "# print('Ranking :')\n",
    "# for key,value in cobain('doubt zone').items():\n",
    "#     print(key + \": \" ,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc 001': 1.0, 'doc 257': 0.7333756110162766, 'doc 422': 0.6798236632881376}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cobain ('doubt zone')\n",
    "asd = result[0]\n",
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.7333756110162766\n",
      "0.6798236632881376\n"
     ]
    }
   ],
   "source": [
    "for key,value in asd.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(kueri, limit):\n",
    "    df_tfidf = pd.read_csv('Konstruksi Indeks.csv')\n",
    "    doc = df_tfidf['Documents']\n",
    "    term = df_tfidf['Term']\n",
    "    df_tfidf\n",
    "    max_range = len(term)\n",
    "\n",
    "    query = kueri\n",
    "    text, data = preprocess(query)\n",
    "    \n",
    "    z=1\n",
    "    count = 1\n",
    "    N = 500\n",
    "    plus = 0\n",
    "    list_angka = list()\n",
    "    list_nama = list()\n",
    "    list_final = list()\n",
    "    list_finalnama = list()\n",
    "\n",
    "    list_wnama = list()\n",
    "    list_wangka = list()\n",
    "    list_simnama = list()\n",
    "    list_simdoc = list()\n",
    "    list_simq = list()\n",
    "    list_simqnama = list()\n",
    "    list_sim = list()\n",
    "    list_finalsim = list()\n",
    "    list_finalsim2 = list()\n",
    "    list_finalsim2doc = list()\n",
    "\n",
    "    dict_w = dict()\n",
    "    dict_f = dict()\n",
    "    dict_tf = dict()\n",
    "    dict_idf = dict()\n",
    "    dict_dj = dict()\n",
    "    dict_dj2 = dict()\n",
    "    dict_sim = dict()\n",
    "\n",
    "    list_rank = list()\n",
    "    counter = 0\n",
    "\n",
    "    starts = timeit.default_timer()\n",
    "    \n",
    "    for q in data:\n",
    "        if data[counter]==data[counter-1]:\n",
    "            plus+=1\n",
    "            dict_f[q] = plus\n",
    "        else:\n",
    "            plus = 1\n",
    "            dict_f[q] = plus\n",
    "        counter+=1\n",
    "\n",
    "    for q in data:\n",
    "        for i in range (0,max_range):\n",
    "            ni = dict_f[q]\n",
    "            start = 2\n",
    "            end = 5\n",
    "            count = 1\n",
    "            if (term[i] == q):\n",
    "                for y in range (0,int(len(doc[i])/7)):\n",
    "                    if(int(len(doc[i])/7)!=1):\n",
    "                        if(doc[i][start:end] != doc[i][(start-7):(end-7)]):\n",
    "                            ni+=1\n",
    "                            dict_f[q +' doc ' + doc[i][start:end]]=count\n",
    "                        else:\n",
    "                            dict_f[q + ' doc' + doc[i][start:end]]=count\n",
    "                            count+=1\n",
    "                            ni=1\n",
    "                    else:\n",
    "                        dict_f[q +' doc ' + doc[i][start:end]]=count\n",
    "                        ni+=1\n",
    "\n",
    "                    start = start+7\n",
    "                    end = end+7\n",
    "\n",
    "                dict_idf[q] = math.log((1+N/ni),10)\n",
    "\n",
    "    for key in dict_f:\n",
    "        dict_tf[key] = 1+math.log(dict_f[key],10)\n",
    "\n",
    "\n",
    "    for key in dict_tf:\n",
    "        for key2 in dict_idf:\n",
    "            if key[0:len(key)-8] == key2:\n",
    "                dict_w[key] = dict_tf[key]*dict_idf[key2]\n",
    "            elif key == key2:\n",
    "                dict_w[key] = dict_tf[key]*dict_idf[key2]\n",
    "\n",
    "    for key in dict_w:\n",
    "        if(key[len(key)-7:len(key)-4] == 'doc' ):\n",
    "            dict_dj[key[len(key)-7:len(key)]] = (pow(dict_w[key],2))\n",
    "            list_angka.append(dict_dj[key[len(key)-7:len(key)]])\n",
    "            list_nama.append(key[len(key)-7:len(key)])\n",
    "        else:\n",
    "            dict_dj[key] = pow(dict_w[key],2)\n",
    "            list_nama.append('q')\n",
    "            list_angka.append(dict_dj[key])\n",
    "\n",
    "    for i in range(0,len(list_nama)):\n",
    "        for x in range(i+1,len(list_nama)):\n",
    "            if (list_nama[i]==list_nama[x]):\n",
    "\n",
    "                list_final.append(math.sqrt(list_angka[i]+list_angka[x]))\n",
    "                list_finalnama.append(list_nama[i])\n",
    "#                 list_angka[i]=0\n",
    "#                 list_angka[x] = 0\n",
    "\n",
    "    for i in range(0,len(list_nama)):\n",
    "        if list_angka[i]!=0:\n",
    "            list_final.append(math.sqrt(list_angka[i]))\n",
    "            list_finalnama.append(list_nama[i])\n",
    "\n",
    "    for i in range(0,len(list_final)):\n",
    "        dict_dj2[list_finalnama[i]] = list_final[i]\n",
    "\n",
    "\n",
    "    for key in dict_w:\n",
    "        list_wnama.append(key)\n",
    "        list_wangka.append(dict_w[key])\n",
    "\n",
    "    for i in range(0,len(list_wnama)):\n",
    "        for x in range(i+1,len(list_wnama)):\n",
    "            if(list_wnama[i]==list_wnama[x][0:len(list_wnama[x])-8]):\n",
    "                list_simnama.append(list_wnama[x][0:len(list_wnama[x])-8])\n",
    "                list_simdoc.append(list_wnama[x][len(list_wnama[x])-7:])\n",
    "                list_sim.append(list_wangka[x])\n",
    "                list_wangka[x] = 0\n",
    "\n",
    "    for i in range(0,len(list_wangka)):\n",
    "        if(list_wangka[i]!=0):\n",
    "            list_simq.append(list_wangka[i])\n",
    "            list_simqnama.append(list_wnama[i])\n",
    "\n",
    "    for i in range(0,len(list_sim)):\n",
    "        for y in range(0,len(list_simq)):\n",
    "            if(list_simnama[i]==list_simqnama[y]):\n",
    "                list_finalsim.append(list_sim[i]*list_simq[y])\n",
    "\n",
    "    for i in range(0,len(list_finalsim)):\n",
    "        for x in range(i+1,len(list_simdoc)):\n",
    "            if list_simdoc[i]==list_simdoc[x]:\n",
    "                list_finalsim2.append(list_finalsim[i]+list_finalsim[x])\n",
    "                list_finalsim[i] = 0\n",
    "                list_finalsim[x] = 0\n",
    "                list_finalsim2doc.append(list_simdoc[i])\n",
    "\n",
    "    for i in range(0,len(list_finalsim)):\n",
    "        if(list_finalsim[i]!=0):\n",
    "            list_finalsim2.append(list_finalsim[i])\n",
    "            list_finalsim2doc.append(list_simdoc[i])\n",
    "\n",
    "    for i in range(0,len(list_finalsim2)):\n",
    "        try:\n",
    "            dict_sim[list_finalsim2doc[i]] = list_finalsim2[i]/(dict_dj2[list_finalsim2doc[i]]*dict_dj2['q'])\n",
    "        except:\n",
    "            print(\"0\")\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    return dict_sim, stop-starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('018', 2.0), ('255', 1.7884418607330084), ('195', 1.7578537947006563), ('227', 1.7268479048030358), ('494', 1.7217996455951832), ('438', 1.721769846148814), ('492', 1.7052857753601731), ('028', 1.624196350581785), ('061', 1.5623818557528477), ('254', 1.482495128333041), ('194', 1.4749727956397112), ('208', 1.4725253605886368), ('446', 1.4548966821755556), ('495', 1.4447205347896634), ('276', 1.4411249762822023), ('154', 1.4379314230745375), ('382', 1.4357167941037234), ('269', 1.4273078347991526), ('153', 1.3857245406518233), ('133', 1.3096977565240242), ('150', 1.2964762856639356), ('498', 1.0), ('496', 1.0), ('491', 1.0), ('488', 1.0), ('484', 1.0), ('463', 1.0), ('458', 1.0), ('457', 1.0), ('455', 1.0), ('454', 1.0), ('451', 1.0), ('448', 1.0), ('447', 1.0), ('445', 1.0), ('444', 1.0), ('441', 1.0), ('430', 1.0), ('425', 1.0), ('420', 1.0), ('409', 1.0), ('405', 1.0), ('404', 1.0), ('392', 1.0), ('386', 1.0), ('383', 1.0), ('378', 1.0), ('376', 1.0), ('375', 1.0), ('372', 1.0), ('367', 1.0), ('355', 1.0), ('350', 1.0), ('343', 1.0), ('340', 1.0), ('339', 1.0), ('338', 1.0), ('330', 1.0), ('328', 1.0), ('316', 1.0), ('314', 1.0), ('306', 1.0), ('303', 1.0), ('292', 1.0), ('288', 1.0), ('284', 1.0), ('279', 1.0), ('268', 1.0), ('267', 1.0), ('265', 1.0), ('263', 1.0), ('262', 1.0), ('261', 1.0), ('259', 1.0), ('256', 1.0), ('246', 1.0), ('242', 1.0), ('235', 1.0), ('234', 1.0), ('233', 1.0), ('231', 1.0), ('230', 1.0), ('224', 1.0), ('216', 1.0), ('214', 1.0), ('207', 1.0), ('203', 1.0), ('201', 1.0), ('197', 1.0), ('180', 1.0), ('178', 1.0), ('174', 1.0), ('169', 1.0), ('161', 1.0), ('155', 1.0), ('148', 1.0), ('145', 1.0), ('138', 1.0), ('136', 1.0), ('134', 1.0), ('116', 1.0), ('114', 1.0), ('102', 1.0), ('078', 1.0), ('069', 1.0), ('060', 1.0), ('052', 1.0), ('047', 1.0), ('043', 1.0), ('040', 1.0), ('026', 1.0), ('012', 1.0), ('001', 1.0), ('419', 0.7779617734035771), ('371', 0.777961773403577), ('324', 0.777961773403577), ('318', 0.777961773403577), ('232', 0.777961773403577), ('204', 0.777961773403577), ('156', 0.777961773403577), ('439', 0.7779617734035769), ('188', 0.7779617734035769), ('135', 0.7779617734035769), ('111', 0.7779617734035769), ('095', 0.7779617734035769), ('399', 0.0), ('322', 0.0), ('107', 0.0), ('098', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "import xml.dom.minidom\n",
    "\n",
    "def keula(kueri, limit):\n",
    "    query = kueri\n",
    "    limit = limit\n",
    "    user_input = query.lower()\n",
    "    docs_results = tfidf(user_input, limit)\n",
    "\n",
    "    times = docs_results[1]\n",
    "\n",
    "    documents = docs_results[0]\n",
    "    founded = len(documents)\n",
    "\n",
    "    result_dict = list()\n",
    "    results = []\n",
    "\n",
    "    for key, value in documents.items():\n",
    "        result_dict.append((key[4:], value))\n",
    "        result_sorted = sorted(result_dict, key = \n",
    "             lambda kv:(kv[1], kv[0]), reverse = True)   \n",
    "    \n",
    "    for key, value in result_sorted:\n",
    "        dictionary = dict()\n",
    "        filename = 'Doc0' + key + '.xml'\n",
    "        doc = xml.dom.minidom.parse('XML/' + filename)\n",
    "        title = doc.getElementsByTagName('TITLE')\n",
    "        title = title[0].firstChild.nodeValue\n",
    "\n",
    "        date = doc.getElementsByTagName('DATE')\n",
    "        date = date[0].firstChild.nodeValue\n",
    "        body = doc.getElementsByTagName('BODY')\n",
    "        body = body[0].firstChild.nodeValue\n",
    "        dictionary['document_no'] = key\n",
    "        dictionary['score'] = value\n",
    "        dictionary['title'] = title\n",
    "        dictionary['date'] = date\n",
    "        dictionary['body'] = body\n",
    "        results.append(dictionary)\n",
    "#         print(results)\n",
    "        \n",
    "#     for key, value in result_sorted:\n",
    "#         results.append(key)\n",
    "#         print(key)\n",
    "#         results.append(value)\n",
    "        \n",
    "    print(result_sorted)\n",
    "        \n",
    "keula('chairman closed the last', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(query, limit, lambd):\n",
    "    clean_query_text, clean_query_words = preprocess(query)\n",
    "    result_dict = list()\n",
    "\n",
    "    df_doc_freq = pd.read_csv('Document Frequencies.csv')\n",
    "    df = pd.read_csv('Frequencies.csv')\n",
    "\n",
    "    global_freq = int(open(\"global_frequency.txt\", \"r\").read())\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    for i in df_doc_freq.iterrows():\n",
    "        local_freq = i[1]['Frequency']\n",
    "        doc_number = i[1]['Document']\n",
    "        doc_number = str(doc_number).zfill(3)\n",
    "        result = 1\n",
    "        else_terus = True\n",
    "        for j in clean_query_words:\n",
    "            entries = df.loc[df['Term'] == j]['Documents'].values[0]\n",
    "            entries = entries.split(', ', 1)\n",
    "            entries[0] = entries[0].replace(\"[\", \"\")\n",
    "            entries[1] = entries[1].replace(']', '')\n",
    "            entries[1] = ast.literal_eval(entries[1])\n",
    "            if doc_number in entries[1].keys():\n",
    "                prob = (float(entries[1][doc_number])/float(local_freq) *\n",
    "                        lambd) + (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "                else_terus = False\n",
    "            else:\n",
    "                prob = (float(entries[0])/float(global_freq)*(1-lambd))\n",
    "            result *= prob\n",
    "        if(else_terus):\n",
    "            continue\n",
    "        # if len(result_dict) > 0 and result_dict[-1][1]-result > 1:\n",
    "        #     break\n",
    "        result_dict.append((doc_number, result))\n",
    "    result_sorted = sorted(result_dict, key=lambda x: x[1], reverse=True)\n",
    "    stop = timeit.default_timer()\n",
    "    if limit > len(result_sorted):\n",
    "        limit = len(result_sorted)\n",
    "    return result_sorted[:limit], stop-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 3; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-9cc44badcc6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m \u001b[0mteuing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doubt zone'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-142-9cc44badcc6b>\u001b[0m in \u001b[0;36mteuing\u001b[1;34m(kueri, lim, lam)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mdosc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdosc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
     ]
    }
   ],
   "source": [
    "def teuing(kueri, lim, lam):\n",
    "    query = kueri\n",
    "    limit = lim\n",
    "    lambdaa = lam\n",
    "    user_input = query.lower()\n",
    "\n",
    "    docs_results = language_model(user_input, limit, lambdaa)\n",
    "    founded = len(docs_results[0])\n",
    "    times = docs_results[1]\n",
    "\n",
    "    documents = docs_results[0]\n",
    "\n",
    "    result_dict = list()\n",
    "    results = []\n",
    "\n",
    "#     for key, value in documents:\n",
    "#         result_sorted = sorted(documents, key = \n",
    "#             lambda kv:(kv[1], kv[0]))\n",
    "    \n",
    "    for key, value in dosc:\n",
    "        result_dict.append((key, value))\n",
    "        result_sorted = sorted(\n",
    "            result_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "\n",
    "    results = []\n",
    "    # read xml\n",
    "    for key, value in result_sorted:\n",
    "        dictionary = dict()\n",
    "        filename = 'Doc0' + key + '.xml'\n",
    "        doc = xml.dom.minidom.parse('XML/' + filename)\n",
    "        title = doc.getElementsByTagName('TITLE')\n",
    "        title = title[0].firstChild.nodeValue\n",
    "\n",
    "        date = doc.getElementsByTagName('DATE')\n",
    "        date = date[0].firstChild.nodeValue\n",
    "        body = doc.getElementsByTagName('BODY')\n",
    "        body = body[0].firstChild.nodeValue\n",
    "\n",
    "        dictionary['document_no'] = key\n",
    "        dictionary['score'] = value\n",
    "        dictionary['title'] = title\n",
    "        dictionary['date'] = date\n",
    "        dictionary['body'] = body\n",
    "        results.append(dictionary)\n",
    "\n",
    "    return result_sorted\n",
    "\n",
    "\n",
    "teuing('doubt zone', 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
